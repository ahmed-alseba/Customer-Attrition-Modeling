{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types_X = {'z_age': 'float64', 'z_census_household_1p_pct': 'float64', 'z_census_education_high_pct': 'float64', \n",
    "                'z_census_purchase_household': 'float64', 'z_census_purchase_capita': 'float64', \n",
    "                'z_census_household_cnt': 'float64', 'multiplay_cnt': 'float64', 'z_line_cnt': 'float64', \n",
    "                'z_sim_cnt': 'float64', 'fixed_prod_cat1_ind': 'float64', 'tenure_fixed_month': 'float64', \n",
    "                'tenure_mobile_month': 'float64', 'z_line_voice_cat1_cnt': 'float64', 'fixed_data_cat1_ind': 'float64', \n",
    "                'fixed_data_cat2_ind': 'float64', 'z_fixed_prod_cat2_cnt': 'float64', 'z_fixed_prod_cat1_cnt': 'float64', \n",
    "                'z_fixed_data_cat3_cnt': 'float64', 'fixed_prod_cat3_cnt': 'float64', 'device_smartphone_cnt': 'float64', \n",
    "                'z_mobile_voice_cat1_cnt': 'float64', 'z_mobile_data_cat1_cnt': 'float64', 'mobile_data_cat2_cnt': 'float64', \n",
    "                'z_mobile_voice_cat3_cnt': 'float64', 'z_mobile_data_cat3_cnt': 'float64', 'z_usg_fv_3m_avg': 'float64', \n",
    "                'z_usg_fd_mb_1m_sum': 'float64', 'z_usg_fd_mb_3m_avg': 'float64', 'z_usg_mv_ib_a_3m_avg': 'float64', \n",
    "                'z_usg_md_sms_ib_a_3m_avg': 'float64', 'z_usg_md_ib_mb_3m_avg': 'float64', \n",
    "                'payment_method_cash_cnt': 'float64', 'z_rev_1m_sum': 'float64', 'z_device_netcube_cnt': 'float64', \n",
    "                'z_tariff_netcube_cnt': 'float64', 'z_min_Prog_Max_BB_Down': 'float64', 'z_line_Fib2h_CNT': 'float64', \n",
    "                'z_min_Speed_Product_KBit': 'float64', 'z_Max_Speed_Missing_KBit': 'float64', \n",
    "                'z_Min_Speed_Reserve_KBit': 'float64', 'z_Max_DSL_OOS_PCT': 'float64', 'z_PR_Relocation_CNT': 'float64', \n",
    "                'z_PR_Relocation_Days': 'float64', 'z_PR_ActivationSupportOpt_CNT': 'float64', \n",
    "                'z_PR_ActivationSupportOpt_Days': 'float64', 'z_PR_DeactivationThreat_CNT': 'float64', \n",
    "                'z_PR_DeactivationSupport_CNT': 'float64', 'z_PR_DeactivationProdOpt_CNT': 'float64', \n",
    "                'z_PR_DeactivationProdOpt_Days': 'float64', 'z_PR_OtherWOTopic_CNT': 'float64', \n",
    "                'z_PR_OtherWOTopic_Days': 'float64', 'z_PR_AddressChange_CNT': 'float64', \n",
    "                'z_PR_AddressChange_Days': 'float64', 'z_PR_ServiceDisruption_CNT': 'float64', \n",
    "                'z_PR_ServiceDisruption_Days': 'float64', 'z_PR_BasketSupport_CNT': 'float64', \n",
    "                'z_PR_BasketSupport_Days': 'float64', 'z_PR_SellingSalesSupport_CNT': 'float64', \n",
    "                'z_PR_SellingSalesSupport_Days': 'float64', 'z_PR_DigitalUsage_CNT': 'float64', \n",
    "                'z_PR_DigitalUsage_Days': 'float64', 'z_TNPS_Last_Days': 'float64', 'z_TNPS_Score_Avg': 'float64', \n",
    "                'province_cd_A': 'float64', 'province_cd_B': 'float64', 'province_cd_C': 'float64', \n",
    "                'province_cd_D': 'float64', 'province_cd_E': 'float64', 'province_cd_F': 'float64', 'province_cd_G': 'float64', \n",
    "                'province_cd_H': 'float64', 'province_cd_I': 'float64', 'Gender_CD_F': 'float64', \n",
    "                'prod_monodual_cd_D': 'float64', 'customer_value_cd_cat': 'float64'}\n",
    "\n",
    "column_types_y = {'target_ind': 'float64'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv', dtype=column_types_X)\n",
    "y_train = pd.read_csv('y_train.csv', dtype=column_types_y)\n",
    "X_train['target_ind'] = y_train['target_ind']\n",
    "\n",
    "X_val = pd.read_csv('X_val.csv', dtype=column_types_X)\n",
    "y_val = pd.read_csv('y_val.csv', dtype=column_types_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8260347869414235"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val['target_ind'].sum()*100/y_val['target_ind'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8259939649028851"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['target_ind'].sum()*100/y_train['target_ind'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80:20 Upsampling of Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate dataset by class\n",
    "df_minority  = X_train[X_train['target_ind']==1]\n",
    "df_majority = X_train[X_train['target_ind']==0]\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority = df_minority.sample(n=int(0.25*len(df_majority)), replace=True)\n",
    "\n",
    "# Merge the two classes into one dataset\n",
    "df = pd.concat([df_majority,df_minority])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1)\n",
    "X_df = df.drop(['target_ind'], axis=1)\n",
    "y_df = df[['target_ind']]\n",
    "\n",
    "X_train = X_train.drop(['target_ind'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost - Max Depth = 2 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=100)\n",
    "ada.fit(X_df, y_df['target_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MCC: \" + str(matthews_corrcoef(y_df['target_ind'], ada.predict(X_df))))\n",
    "print(\"Validation MCC: \" + str(matthews_corrcoef(y_val['target_ind'], ada.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost - Max Depth = 3 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3), n_estimators=100)\n",
    "ada.fit(X_df, y_df['target_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MCC: \" + str(matthews_corrcoef(y_df['target_ind'], ada.predict(X_df))))\n",
    "print(\"Validation MCC: \" + str(matthews_corrcoef(y_val['target_ind'], ada.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost - Max Depth = 4 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4), n_estimators=100)\n",
    "ada.fit(X_df, y_df['target_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MCC: \" + str(matthews_corrcoef(y_df['target_ind'], ada.predict(X_df))))\n",
    "print(\"Validation MCC: \" + str(matthews_corrcoef(y_val['target_ind'], ada.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost - Max Depth = 5 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=100)\n",
    "ada.fit(X_df, y_df['target_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MCC: \" + str(matthews_corrcoef(y_df['target_ind'], ada.predict(X_df))))\n",
    "print(\"Validation MCC: \" + str(matthews_corrcoef(y_val['target_ind'], ada.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost - Max Depth = 6 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=6), n_estimators=100)\n",
    "ada.fit(X_df, y_df['target_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MCC: \" + str(matthews_corrcoef(y_df['target_ind'], ada.predict(X_df))))\n",
    "print(\"Validation MCC: \" + str(matthews_corrcoef(y_val['target_ind'], ada.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaCost Code (From MaatPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble.forest import BaseForest\n",
    "from sklearn.tree.tree import BaseDecisionTree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import (check_random_state,\n",
    "                           check_X_y,\n",
    "                           check_array,\n",
    "                           compute_class_weight,\n",
    "                           column_or_1d)\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "__all__ = ['AdaCost']\n",
    "\n",
    "\n",
    "class AdaCost(AdaBoostClassifier):\n",
    "    \"\"\"\n",
    "    Implementation of the cost sensitive variants of AdaBoost; Adacost and AdaC1-3\n",
    "    Reference: Nikolaou et al Mach Learn (2016) 104:359–384.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1.,\n",
    "                 algorithm=None,\n",
    "                 class_weight='balanced',\n",
    "                 random_state=None):\n",
    "        \"\"\"\n",
    "        :param base_estimator: object, optional (default=DecisionTreeClassifier)\n",
    "               The base estimator from which the boosted ensemble is built.\n",
    "               Support for sample weighting is required, as well as proper 'classes_' and 'n_classes_' attributes.\n",
    "        :param n_estimators: int, optional (default=50)\n",
    "               The maximum number of estimators at which boosting is terminated.\n",
    "               In case of perfect fit, the learning procedure is stopped early.\n",
    "        :param learning_rate: float, optional (default=1.)\n",
    "               Learning rate shrinks the contribution of each classifier by \"learning_rate\".\n",
    "               There is a trade-off between \"learning_rate\" and \"n_estimators\".\n",
    "        :param algorithm: algorithm: {'adacost', 'adac1', 'adac2', 'adac3'}, optional (default='adacost')\n",
    "        :param class_weight: dict, list of dicts, “balanced” or None, default=None\n",
    "               Weights associated with classes in the form {class_label: weight}. The “balanced” mode uses \n",
    "               the values of y to automatically adjust weights inversely proportional to class frequencies \n",
    "               in the input data as n_samples / (n_classes * np.bincount(y))If not given, all classes are\n",
    "               supposed to have weight one. For multi-output problems, a list of dicts can be provided in \n",
    "               the same order as the columns of y.\n",
    "        :param random_state: int, RandomState instance or None, optional (default=None)\n",
    "               If int, random_state is the seed used by the random number generator; If RandomState instance,\n",
    "               random_state is the random number generator; If None, the random number generator is the RandomState\n",
    "               instance used by 'np.random'.\n",
    "        \"\"\"\n",
    "        super(AdaBoostClassifier, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state)\n",
    "\n",
    "        self.algorithm = algorithm\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Build a boosted classifier from the training set (X, y).\n",
    "        :param X:{array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "               Matrix containing the training data.\n",
    "        :param y: array-like, shape (n_samples,)\n",
    "               Corresponding label for each sample in X.\n",
    "        :param sample_weight: array-like of shape = [n_samples], optional\n",
    "               Sample weights. If None, the sample weights are initialized to the class weights\n",
    "        :return: object; Return self\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "\n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                 BaseForest))):\n",
    "            DTYPE = np.float64\n",
    "            dtype = DTYPE\n",
    "            accept_sparse = 'csc'\n",
    "        else:\n",
    "            dtype = None\n",
    "            accept_sparse = ['csr', 'csc']\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n",
    "                         y_numeric=is_regressor(self))\n",
    "\n",
    "        y = self._validate_targets(y)\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to class weights\n",
    "            # assign class weight to each sample index\n",
    "            sample_weight = np.copy(y).astype(float)\n",
    "            for n in range(len(self.classes)):\n",
    "                sample_weight[y == n] = self.class_weight_[n]\n",
    "        else:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "        # Normalize existing weights\n",
    "        #sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "        # Check that the sample weights sum is positive\n",
    "        if sample_weight.sum() <= 0:\n",
    "            raise ValueError(\n",
    "                \"Attempting to fit with a non-positive weighted number of samples.\")\n",
    "\n",
    "        if self.algorithm is None:\n",
    "            self.algorithm = \"adacost\"\n",
    "\n",
    "        # Check parameters\n",
    "        self._validate_estimator()\n",
    "\n",
    "        # Clear any previous fit results\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            # Boosting step\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                iboost,\n",
    "                X, y,\n",
    "                sample_weight,\n",
    "                random_state)\n",
    "\n",
    "            # Early termination\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _boost(self, iboost, X, y, sample_weight, random_state):\n",
    "        \"\"\"\n",
    "        Implement a single boost.\n",
    "        Perform a single boost according to the algorithm selected and return the updated\n",
    "        sample weights.\n",
    "        :param iboost: int\n",
    "               The index of the current boost iteration\n",
    "        :param X:{array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "               Matrix containing the training data.\n",
    "        :param y: array-like, shape (n_samples,)\n",
    "               Corresponding label for each sample in X.\n",
    "        :param sample_weight: array-like of shape = [n_samples], optional\n",
    "               Sample weights. If None, the sample weights are initialized to the class weights\n",
    "        :param random_state: int, RandomState instance or None, optional (default=None)\n",
    "               If int, random_state is the seed used by the random number generator; If RandomState instance,\n",
    "               random_state is the random number generator; If None, the random number generator is the RandomState\n",
    "               instance used by 'np.random'.\n",
    "        :return: sample_weight {array-like of shape = [n_samples]}, estimator_weight {float}, estimator_error {float}\n",
    "                Returns updates values for sample weights, estimator weight and estimator error\n",
    "        \"\"\"\n",
    "\n",
    "        estimator = self._make_estimator(random_state=random_state)\n",
    "\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        y_predict = estimator.predict(X)\n",
    "\n",
    "        if iboost == 0:\n",
    "            self.classes_ = getattr(estimator, 'classes_', None)\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "\n",
    "            # assign class weight to each sample index\n",
    "            costs = np.copy(y).astype(float)\n",
    "            for n in range(self.n_classes_):\n",
    "                costs[y == n] = self.class_weight_[n]\n",
    "            self.cost_ = costs\n",
    "        # Instances incorrectly classified\n",
    "        incorrect = y_predict != y\n",
    "\n",
    "        # Error fraction\n",
    "        if self.algorithm == \"adacost\":\n",
    "            estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n",
    "        elif self.algorithm in ['adac1', 'adac2']:\n",
    "            estimator_error = np.mean(np.average(incorrect, weights=sample_weight*self.cost_, axis=0))\n",
    "        elif self.algorithm == \"adac3\":\n",
    "            estimator_error = np.mean(np.average(incorrect, weights=sample_weight*np.power(self.cost_, 2), axis=0))\n",
    "        else:\n",
    "            raise ValueError(\"Algorithms 'adacost', 'adac1', 'adac2' and 'adac3' are accepted;\"\\\n",
    "                             \" got {0}\".format(self.algorithm))\n",
    "        # Stop if classification is perfect\n",
    "        if estimator_error <= 0:\n",
    "            return sample_weight, 1., 0.\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "\n",
    "        # Stop if the error is at least as bad as random guessing\n",
    "        if estimator_error >= 1. - (1. / n_classes):\n",
    "            self.estimators_.pop(-1)\n",
    "            if len(self.estimators_) == 0:\n",
    "                raise ValueError('BaseClassifier in AdaBoostClassifier '\n",
    "                                 'ensemble is worse than random, ensemble '\n",
    "                                 'can not be fit.')\n",
    "            return None, None, None\n",
    "\n",
    "        # Boost weight based on algorithm (Nikolaou et al Mach Learn (2016) 104:359–384)\n",
    "        if self.algorithm == \"adacost\" or self.algorithm == \"adac2\":\n",
    "            estimator_weight = self.learning_rate * 0.5 * (\n",
    "                np.log((1. - estimator_error) / estimator_error))\n",
    "        elif self.algorithm == \"adac1\":\n",
    "            estimator_weight = self.learning_rate * 0.5 * (\n",
    "                np.log((1 + (1. - estimator_error) - estimator_error) /\n",
    "                       (1 - (1. - estimator_error) + estimator_error)))\n",
    "        elif self.algorithm == \"adac3\":\n",
    "            estimator_weight = self.learning_rate * 0.5 * (\n",
    "                np.log((np.sum(sample_weight*self.cost_) + (1 - estimator_error) - estimator_error) /\n",
    "                       (np.sum(sample_weight*self.cost_) - (1. - estimator_error) + estimator_error)))\n",
    "        # Only boost the weights if it will fit again\n",
    "        if iboost < self.n_estimators - 1:\n",
    "            if self.algorithm == \"adacost\":\n",
    "                beta = np.copy(self.cost_).astype(float)\n",
    "                beta[y == y_predict] = np.array(list(map(lambda x: -0.5 * x + 0.5, self.cost_[y == y_predict])))\n",
    "                beta[y != y_predict] = np.array(list(map(lambda x: 0.5 * x + 0.5, self.cost_[y != y_predict])))\n",
    "                # Only boost positive weights\n",
    "                sample_weight *= np.exp(beta * estimator_weight * incorrect *\n",
    "                                        ((sample_weight > 0) | (estimator_weight < 0)))\n",
    "            elif self.algorithm == \"adac1\":\n",
    "                sample_weight *= np.exp(self.cost_ * estimator_weight * incorrect *\n",
    "                                        ((sample_weight > 0) | (estimator_weight < 0)))\n",
    "            elif self.algorithm == \"adac2\":\n",
    "                sample_weight *= self.cost_ * np.exp(estimator_weight * incorrect *\n",
    "                                                     ((sample_weight > 0) | (estimator_weight < 0)))\n",
    "            elif self.algorithm == \"adac3\":\n",
    "                sample_weight *= self.cost_ * np.exp(self.cost_ * estimator_weight * incorrect *\n",
    "                                                     ((sample_weight > 0) | (estimator_weight < 0)))\n",
    "            else:\n",
    "                raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        \"\"\"\n",
    "        Check the estimator and set the base_estimator_ attribute.\n",
    "        \"\"\"\n",
    "        super(AdaBoostClassifier, self)._validate_estimator(\n",
    "            default=DecisionTreeClassifier(max_depth=1, class_weight=self.class_weight_))\n",
    "\n",
    "    def _validate_targets(self, y):\n",
    "        \"\"\"\n",
    "        Validation of y and class_weight.\n",
    "        :param y: array-like, shape (n_samples,)\n",
    "               Corresponding label for each sample in X.\n",
    "        :return: validated y {array-like, shape (n_samples,)}\n",
    "        \"\"\"\n",
    "        y_ = column_or_1d(y, warn=True)\n",
    "        check_classification_targets(y)\n",
    "        cls, y = np.unique(y_, return_inverse=True)\n",
    "        class_weight_ = compute_class_weight(self.class_weight, cls, y_)\n",
    "        self.class_weight_ = {i: class_weight_[i] for i in range(len(class_weight_))}\n",
    "        if len(cls) < 2:\n",
    "            raise ValueError(\n",
    "                \"The number of classes has to be greater than one; got %d\"\n",
    "                % len(cls))\n",
    "\n",
    "        self.classes = cls\n",
    "\n",
    "        return np.asarray(y, dtype=np.float64, order='C')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for X.\n",
    "        The predicted class of an input sample is computed as the weighted mean\n",
    "        prediction of the classifiers in the ensemble.\n",
    "        :param X: {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "               Matrix containing the input samples.\n",
    "        :return: y_predicted; predicted labels for X\n",
    "        \"\"\"\n",
    "        pred = self.decision_function(X)\n",
    "        # >>> removed binary special case\n",
    "        # if self.n_classes_ == 2:\n",
    "        #    return self.classes_.take(pred == 0, axis=0)\n",
    "        # <<<\n",
    "\n",
    "        return self.classes.take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute the decision function of X\n",
    "        :param X: {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "               Matrix containing the input samples.\n",
    "        :return: score : array, shape = [n_samples, k]\n",
    "                 The decision function of the input samples. The order of outputs is the same of\n",
    "                 that of the 'classes_' attribute.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"n_classes_\")\n",
    "        X = self._check_X(X)\n",
    "\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = sum((estimator.predict(X) == classes).T * w\n",
    "                   for estimator, w in zip(self.estimators_,\n",
    "                                           self.estimator_weights_))\n",
    "\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "        # >>> removed binary special case\n",
    "        # if n_classes == 2:\n",
    "        #    pred[:, 0] *= -1\n",
    "        #    return pred.sum(axis=1)\n",
    "        # <<<\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0. 1.], y=[0. 0. 0. ... 1. 0. 0.] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaCost(algorithm='adacost', base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "        n_estimators=100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adacost = AdaCost(n_estimators=100, base_estimator=DecisionTreeClassifier(max_depth=1) )\n",
    "adacost.fit(X_df, y_df['target_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MCC: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MCC: 0.0\n",
      "Val Acc: 0.018260347869414235\n",
      "[[      0 1158549]\n",
      " [      0   21549]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training MCC: \" + str(matthews_corrcoef(y_df['target_ind'], adacost.predict(X_df))))\n",
    "print(\"Validation MCC: \" + str(matthews_corrcoef(y_val['target_ind'], adacost.predict(X_val))))\n",
    "print(\"Val Acc: \" + str(adacost.score(X_val, y_val['target_ind'])))\n",
    "print(confusion_matrix(y_val['target_ind'], adacost.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adacost = AdaCost(n_estimators=100, base_estimator=DecisionTreeClassifier(max_depth=1))\n",
    "adacost.fit(X_train, y_train['target_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MCC: \" + str(matthews_corrcoef(y_df['target_ind'], adacost.predict(X_df))))\n",
    "print(\"Validation MCC: \" + str(matthews_corrcoef(y_val['target_ind'], adacost.predict(X_val))))\n",
    "print(\"Val Acc: \" + str(adacost.score(X_val, y_val['target_ind'])))\n",
    "print(confusion_matrix(y_val['target_ind'], adacost.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adacost = AdaCost(n_estimators=100, class_weight='balanced')\n",
    "# adacost.fit(X_df, y_df['target_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training MCC: \" + str(matthews_corrcoef(y_df['target_ind'], adacost.predict(X_df))))\n",
    "# print(\"Validation MCC: \" + str(matthews_corrcoef(y_val['target_ind'], adacost.predict(X_val))))\n",
    "# print(\"Val Acc: \" + str(adacost.score(X_val, y_val['target_ind'])))\n",
    "# print(confusion_matrix(y_val['target_ind'], adacost.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adacost = AdaCost(n_estimators=100, class_weight='balanced')\n",
    "# adacost.fit(X_train, y_train['target_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training MCC: \" + str(matthews_corrcoef(y_df['target_ind'], adacost.predict(X_df))))\n",
    "# print(\"Validation MCC: \" + str(matthews_corrcoef(y_val['target_ind'], adacost.predict(X_val))))\n",
    "# print(\"Val Acc: \" + str(adacost.score(X_val, y_val['target_ind'])))\n",
    "# print(confusion_matrix(y_val['target_ind'], adacost.predict(X_val)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
